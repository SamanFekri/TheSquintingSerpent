# ğŸ TheSquintingSerpent (Deep Reinforcement Learning)

<p align="center">
  <img src="media/logo.png" alt="Logo" width="256" height="256">
</p>

A fully featured **Snake game with multiple Deep Q-Learning (DQN) variants**, supporting:

- ğŸ§  Reinforcement Learning (PyTorch)
- ğŸ‘ƒ Food â€œsmellâ€ (relative food position)
- ğŸ´ Hunger mechanism (reward shaping)
- ğŸ§± Custom maps (from `.txt` files)
- ğŸªŸ Wrap / no-wrap borders
- ğŸ® Human play
- ğŸ“º Pygame GUI (centered grid, large window)
- ğŸ’¾ Save / load / resume training
- ğŸ† Best model & last model saving

### ğŸ“‚ Agent variants in this repo
- **`heuristic/`** â€“ baseline DQN with **3-channel local vision**, **hunger scalar**, and **food â€œsmellâ€ vector**.
- **`h2/`** â€“ same sensors as the baseline but **compresses vision to a single encoded grid channel**.
- **`h3/`** â€“ H2-style sensors with the snake body **linearly encoded from tail (-0.01) to head (-0.99)** for clearer ordering.
- **`lidar/`** â€“ adds **lidar rays** for wall/body distances on top of the baseline observation space.
- **`bfs/`** â€“ keeps the lidar observation space and layers in **BFS-based reward shaping** for safety.

Each folder ships its own README with usage and a quick rundown of the features that specific model uses.

> ğŸ§­ **Design note**: All agents learn from a **limited, first-person vision window** around the snakeâ€™s head. Constraining visibility keeps
> policies from overfitting to a single map layout and makes the agent play like a snake wouldâ€”navigating locallyâ€”rather than from the
> third-person, full-map view humans get in classic Snake.
>
> This small observation window also keeps the convolutional encoder **compact (fewer parameters)**, so models train faster while maintaining
> strong, map-agnostic performance.

---

## ğŸ“ Project Structure
```
.
â”œâ”€â”€ heuristic/   # Baseline DQN agent with 3-channel vision, hunger, smell
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ play.py
â”‚   â”œâ”€â”€ dqn_agent.py
â”‚   â””â”€â”€ snake_game.py
â”œâ”€â”€ h2/          # Vision-compressed variant sharing the same sensors
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ play.py
â”‚   â”œâ”€â”€ dqn_agent.py
â”‚   â””â”€â”€ snake_game.py
â”œâ”€â”€ h3/          # Gradient-encoded body values (tail -0.01 â†’ head -0.99)
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ play.py
â”‚   â”œâ”€â”€ dqn_agent.py
â”‚   â””â”€â”€ snake_game.py
â”œâ”€â”€ lidar/       # Adds lidar distance rays to the observation
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ play.py
â”‚   â”œâ”€â”€ dqn_agent.py
â”‚   â””â”€â”€ snake_game.py
â”œâ”€â”€ bfs/         # Lidar inputs plus BFS-based reward shaping
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ play.py
â”‚   â”œâ”€â”€ dqn_agent.py
â”‚   â””â”€â”€ snake_game.py
â”œâ”€â”€ maps/        # Text map files used by all agents
â”œâ”€â”€ media/       # Assets (logo)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Requirements

```bash
pip install pygame torch numpy
```

Python 3.9+ recommended.

---

## ğŸ§± Maps

Maps are simple text files:
- 0 â†’ free cell
- 1 â†’ wall

Rectangular grid

Size of the map defines the game size

--- 

## ğŸ§  AI Observation Space

Each step the AI receives:

1. Local vision grid (3, 2N+1, 2N+1)
- Channel 0: walls
- Channel 1: snake body
- Channel 2: food
2. Hunger âˆˆ [0, 1]
3. Smell vector (dx, dy)
- Relative distance from head â†’ food
- Normalized
- Wrap-aware (shortest distance)
```python
Observation = (grid, hunger, smell)
```

---

## ğŸ¯ Actions
```ini
0 = UP
1 = DOWN
2 = LEFT
3 = RIGHT
```

âš ï¸ Reverse direction is blocked (classic Snake behavior).
The snake can never move into its neck, even during training.

---

## â˜ ï¸ Game Over Rules

The game ends if the snake:
- âŒ Hits a wall
- âŒ Hits its own body
Hunger does not kill directly â€” it only adds increasing negative reward.

---

## ğŸ… Rewards

- +1.0 â†’ eat food
- âˆ’1.0 â†’ die (wall or self)
- âˆ’0.01 â†’ per step
- extra penalty proportional to hunger

This encourages:
- faster food seeking
- less wandering
- stable learning

---

## ğŸ§ª Training the AI

Basic training (with GUI)
```bash
python train.py \
  --map maps/map_10x10.txt \
  --N 2 \
  --render \
  --wrap \
  --games 5000
```

No wrap (classic borders)
```bash
python train.py \
  --map maps/map_10x10.txt \
  --N 2 \
  --render \
  --no-wrap \
  --games 5000
```

---

## â–¶ï¸ Resume Training (IMPORTANT)

Training automatically saves a full checkpoint.
To continue training from where you stopped:

```bash
python train.py \
  --map maps/map_10x10.txt \
  --N 2 \
  --render \
  --wrap \
  --games 2000 \
  --resume <path-to-model>
```

This restores:
- model weights
- target network
- optimizer state
- episode counter
- best score

---

## ğŸ’¾ Saved Models

| File                   | Meaning             |
| ---------------------- | ------------------- |
| `models/best.pt`       | Best score achieved |
| `models/last.pt`       | Last episode        |
| `models/checkpoint.pt` | Resume training     |

---

## ğŸ® Play the Game

Watch AI play
```bash
python play.py \
  --map maps/map_10x10.txt \
  --N 2 \
  --wrap \
  --model models/best.pt
```

Play as human
```bash
python play.py \
  --map maps/map_10x10.txt \
  --human \
  --wrap
```

Controls:
- â¬†ï¸ Up
- â¬‡ï¸ Down
- â¬…ï¸ Left
- â¡ï¸ Right

---

ğŸ“º GUI Features

- Large window
- Game grid centered
- HUD outside the grid
- Live training info:
  - episode
  - score
  - best score
  - epsilon
  - loss
  - checkpoint path

---

## ğŸ§  Design Decisions (Why it works well)
âœ” No reverse action â†’ cleaner action space
âœ” Relative observations â†’ translation invariant
âœ” Smell vector â†’ faster convergence
âœ” Hunger shaping â†’ prevents infinite loops
âœ” Resume training â†’ practical for long runs

---

## Advantages of NxN Sight + Smell in Snake AI

- **Generalization**: The agent learns local patterns, not fixed map layouts.
- **Scalability**: Works on any map size without changing the network.
- **Faster Learning**: Smaller state space â†’ more stable and efficient training.
- **Robustness**: Partial observability prevents brittle, map-specific strategies.
- **Goal Awareness**: Smell (dx, dy) gives direction without solving the path.
- **Natural Behavior**: Produces snake-like movement instead of optimal but unnatural paths.
- **Transferability**: Same policy works on unseen maps and different environments.
- **Realistic Design**: Mirrors real agents (local sensors + goal direction).
- **Reduced Overfitting**: No absolute positions or full-map shortcuts.
- **Clean Action Space**: Encourages anticipation rather than memorization.
- **Small, efficient networks**: Limited vision keeps the CNN tiny, reducing compute without sacrificing performance.


---

## ğŸš€ Possible Extensions
- Add body-relative offsets (top-K segments)
- Add danger flags (up/down/left/right)
- LSTM for memory
- Curriculum maps (easy â†’ hard)
- Map editor in pygame
- Imitation learning from human play

