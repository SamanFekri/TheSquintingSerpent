# ğŸ TheSquintingSerpent (Deep Reinforcement Learning)

<p align="center">
  <img src="media/logo.png" alt="Logo" width="256" height="256">
</p>

A fully featured **Snake game with a Deep Q-Learning (DQN) agent**, supporting:

- ğŸ§  Reinforcement Learning (PyTorch)
- ğŸ‘ƒ Food â€œsmellâ€ (relative food position)
- ğŸ´ Hunger mechanism (reward shaping)
- ğŸ§± Custom maps (from `.txt` files)
- ğŸªŸ Wrap / no-wrap borders
- ğŸ® Human play
- ğŸ“º Pygame GUI (centered grid, large window)
- ğŸ’¾ Save / load / resume training
- ğŸ† Best model & last model saving

---

## ğŸ“ Project Structure
```
.
â”œâ”€â”€ snake_game.py # Game environment + pygame renderer
â”œâ”€â”€ dqn_agent.py # DQN model, replay buffer, checkpoints
â”œâ”€â”€ train.py # Training loop (with resume support)
â”œâ”€â”€ play.py # Play as human or watch trained AI
â”œâ”€â”€ maps/
â”‚ â””â”€â”€ map_*.txt # Custom maps (0 = free, 1 = wall)
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ best.pt # Best model by score
â”‚ â”œâ”€â”€ last.pt # Last episode model
â”‚ â””â”€â”€ checkpoint.pt # Full checkpoint (resume training)
â””â”€â”€ README.md
```

---

## âš™ï¸ Requirements

```bash
pip install pygame torch numpy
```

Python 3.9+ recommended.

---

## ğŸ§± Maps

Maps are simple text files:
- 0 â†’ free cell
- 1 â†’ wall

Rectangular grid

Size of the map defines the game size

--- 

## ğŸ§  AI Observation Space

Each step the AI receives:

1. Local vision grid (3, 2N+1, 2N+1)
- Channel 0: walls
- Channel 1: snake body
- Channel 2: food
2. Hunger âˆˆ [0, 1]
3. Smell vector (dx, dy)
- Relative distance from head â†’ food
- Normalized
- Wrap-aware (shortest distance)
```python
Observation = (grid, hunger, smell)
```

---

## ğŸ¯ Actions
```ini
0 = UP
1 = DOWN
2 = LEFT
3 = RIGHT
```

âš ï¸ Reverse direction is blocked (classic Snake behavior).
The snake can never move into its neck, even during training.

---

## â˜ ï¸ Game Over Rules

The game ends if the snake:
- âŒ Hits a wall
- âŒ Hits its own body
Hunger does not kill directly â€” it only adds increasing negative reward.

---

## ğŸ… Rewards

- +1.0 â†’ eat food
- âˆ’1.0 â†’ die (wall or self)
- âˆ’0.01 â†’ per step
- extra penalty proportional to hunger

This encourages:
- faster food seeking
- less wandering
- stable learning

---

## ğŸ§ª Training the AI

Basic training (with GUI)
```bash
python train.py \
  --map maps/map_10x10.txt \
  --N 2 \
  --render \
  --wrap \
  --games 5000
```

No wrap (classic borders)
```bash
python train.py \
  --map maps/map_10x10.txt \
  --N 2 \
  --render \
  --no-wrap \
  --games 5000
```

---

## â–¶ï¸ Resume Training (IMPORTANT)

Training automatically saves a full checkpoint.
To continue training from where you stopped:

```bash
python train.py \
  --map maps/map_10x10.txt \
  --N 2 \
  --render \
  --wrap \
  --games 2000 \
  --resume <path-to-model>
```

This restores:
- model weights
- target network
- optimizer state
- episode counter
- best score

---

## ğŸ’¾ Saved Models

| File                   | Meaning             |
| ---------------------- | ------------------- |
| `models/best.pt`       | Best score achieved |
| `models/last.pt`       | Last episode        |
| `models/checkpoint.pt` | Resume training     |

---

## ğŸ® Play the Game

Watch AI play
```bash
python play.py \
  --map maps/map_10x10.txt \
  --N 2 \
  --wrap \
  --model models/best.pt
```

Play as human
```bash
python play.py \
  --map maps/map_10x10.txt \
  --human \
  --wrap
```

Controls:
- â¬†ï¸ Up
- â¬‡ï¸ Down
- â¬…ï¸ Left
- â¡ï¸ Right

---

ğŸ“º GUI Features

- Large window
- Game grid centered
- HUD outside the grid
- Live training info:
  - episode
  - score
  - best score
  - epsilon
  - loss
  - checkpoint path

---

## ğŸ§  Design Decisions (Why it works well)
âœ” No reverse action â†’ cleaner action space
âœ” Relative observations â†’ translation invariant
âœ” Smell vector â†’ faster convergence
âœ” Hunger shaping â†’ prevents infinite loops
âœ” Resume training â†’ practical for long runs

---

## Advantages of NxN Sight + Smell in Snake AI

- **Generalization**: The agent learns local patterns, not fixed map layouts.
- **Scalability**: Works on any map size without changing the network.
- **Faster Learning**: Smaller state space â†’ more stable and efficient training.
- **Robustness**: Partial observability prevents brittle, map-specific strategies.
- **Goal Awareness**: Smell (dx, dy) gives direction without solving the path.
- **Natural Behavior**: Produces snake-like movement instead of optimal but unnatural paths.
- **Transferability**: Same policy works on unseen maps and different environments.
- **Realistic Design**: Mirrors real agents (local sensors + goal direction).
- **Reduced Overfitting**: No absolute positions or full-map shortcuts.
- **Clean Action Space**: Encourages anticipation rather than memorization.


---

## ğŸš€ Possible Extensions
- Add body-relative offsets (top-K segments)
- Add danger flags (up/down/left/right)
- LSTM for memory
- Curriculum maps (easy â†’ hard)
- Map editor in pygame
- Imitation learning from human play

